\chapter{Conclusion} \label{chap:conc}

In the previous chapter, we summarized our evaluation of the \textit{Proactive} flow scheduler in terms of its  performance and compared it with ECMP routing and Global First-Fit flow scheduling. In this chapter, we will summarize the project and our results, and discuss possible future work.

\section{Project Overview}

Recent years have seen a steady growth in the amount of data generated by mobile and web applications, which is the fuel that drives Big Data Analytics. Mayer-Sch{\"o}nberger \textit{et al.} \cite{mayer2013big} define "big data" as the ability of harnessing information in novel ways thereby producing valuable insights or goods of significant value. As the data volumes grow, big data processing frameworks such as Hadoop \cite{HadoopWeb} require scaling out to thousands of commodity servers, subsequently resulting in an increase in the network traffic. Network performance has been found out to be of paramount importance for optimizing the processing times of Big Data applications, since research \cite{al2010hedera, greenberg2009vl2,guo2008dcell} has determined the network to be a performance bottleneck.

Recent studies \cite{al2008scalable, greenberg2009vl2, guo2008dcell,guo2009bcube} propose horizontal scaling of hosts to thousands of commodity servers in multi-rooted tree topologies, such as a \textit{fat-tree} topology that exploit path diversity to overcome limited port densities in commodity switches, thereby scaling the network with the increasing number of servers required to process large volumes of data. Moreover, emergent technologies such as Software-Defined Networking enable programming of the network stack, by maintaining a global view of the network state which is enabled by the separation of the control plane from the data plane of forwarding devices \cite{lantz2010network}. 

Various research efforts \cite{das2013transparent, wang2012programming, al2010hedera, narayan2012hadoop, neves2014pythia, wette2015hybridte} have tried to make the network dynamically reconfigure according to application traffic demands, in order to avoid congestion and optimize big data processing in a data centre network. Dynamic network reconfiguration approaches are \textit{reactive} in nature and try to optimize network performance by either being \textit{application-aware}, where the big data application controller reports traffic demands to the network controller which subsequently reconfigures the network accordingly, or by adopting the approach of \textit{traffic-awareness}, where a network monitor provides information to the network controller for \textit{reactive} reconfiguration. However, such \textit{reactive} measures of network configuration are bound to induce control traffic in the network and cause high level latencies in reconfiguring the network dynamically. 

We propose a \textit{Proactive} approach for the configuration of a data centre network, which installs flow rules in the network before the big data application starts. The flow rules are obtained from the previous execution of the same application when it is routed by the \textit{Global-First Fit} \cite{al2010hedera} flow scheduling algorithm, which is a \textit{traffic-aware} algorithm that builds upon \textit{Equal Cost Multi-Path} (ECMP) routing. ECMP is the standard routing protocol in multi-rooted data centre networks with path \textit{multiplicity}. Our \textit{Proactive} network controller reverts to Global First-Fit flow scheduling once the \textit{proactive} flows installed by it in the forwarding devices expire. 

In order to evaluate our \textit{Proactive} network controller against ECMP routing and Global First-Fit flow scheduling, we ran emulations of Hadoop jobs on a 16 host \textit{fat-tree} topology, running on a single server, and measured the effect of the different flow scheduling mechanisms on total bisection bandwidth achieved by the hosts in the network and Hadoop job completion times. An average gain of \textbf{59.9}\% and \textbf{11.6}\% in total bisection bandwidth achieved by the hosts in the network in comparison to ECMP routing and Global First-Fit flow scheduling was observed, when network traffic was routed by the \textit{Proactive} controller. Moreover, the \textit{Proactive} controller reduced Hadoop job completion times by \textbf{35.58}\% and \textbf{10.07}\% in comparison to ECMP routing and Global First-Fit flow scheduling. 

Automatic generation of \textit{Proactive} configurations for the network was beyond the scope of this project, therefore we used the network configurations generated by Global First-Fit flow scheduling. The results obtained by us indicate that \textit{proactively} configuring the network results in an increase in the total network utilization which benefits the performance of big data processing applications. Given that current \textit{application-aware} and \textit{traffic-aware} approaches of dynamic network configuration are not optimized with \textit{Proactive} configurations, there may be more to be gained in terms of average network bisection bandwidth utilization, if they are optimized before deployment.  

\section{Contribution}
In summary, our project validates the effectiveness of \textit{Proactive} configuration of data centre networks in optimizing big data application performance. Our evaluation results demonstrate that \textit{reactive} approaches have a lot to gain if they are optimized \textit{proactively} before deployment, since our \textit{Proactive} flow scheduler is able to achieve higher bisection bandwidth utilization in a data centre network and lower Hadoop job completion times than current static ECMP routing and \textit{reactive} approaches explored in research, such as Global First-Fit flow scheduling. 

Furthermore, we established a \textit{high negative correlation} between total bisection bandwidth achieved in the network and Hadoop job completion times, further highlighting the significance of network performance in accelerating big data applications.  

\section{Future Work}

While our \textit{Proactive} flow scheduling approach has increased the network performance in terms of the total bisection bandwidth achieved, it relies on flow scheduling decisions made by the \textit{Global First-Fit} algorithm for the same Hadoop job. In order to fully explore the effect of \textit{Proactive} configuration of the network, the network has to be configured proactively based on the application communication patterns. A future extension to our \textit{Proactive} flow scheduler might be to independently install \textit{Proactive} configurations in the network by generating them automatically, on the basis of application communication patterns.

We based our experiment on an emulation based testbed, since we did not have access to the hardware resources for deploying our implementation on a real cluster of 16 hosts. Moreover, for the same reason, we used the Hadoop job traces obtained by Neves \textit{et al.} \cite{neves2015mremu}, which were not classified according to the MapReduce applications that were run to obtain the traces, thereby hindering our ability to account for the difference in total bisection bandwidth achieved for different Hadoop job traces as discussed in Chapter \ref{chap:eval}. Consequently, a future extension to our experiment would be to obtain Hadoop job traces from a real cluster and subsequently run the experiment on the same, in order to validate the findings from our emulation based testbed and account for the difference in the average bisection bandwidth achieved for different Hadoop applications.   

\section{Final Remarks}

Big Data processing is being used to solve complex problems of society by levering enormous volumes of data available from ubiquitous computing devices. In order to ensure robust performance of big data applications, data centre networks have to be configured according to application communication patterns, so that traffic congestion in the network is avoided. By utilizing the emergent technology of Software-Defined Networking, the network stack can be programmed to configure a data centre network in accordance with application communication patterns, enabling scalability of network performance as more commodity servers are added into the network in order to deal with the growing data volumes. We showed that optimizing the network \textit{proactively} results in performance gains over current \textit{reactive} approaches and hope that as research evolves out in this field, the full potential of \textit{Proactive} network configuration for optimizing big data processing will be exploited.   