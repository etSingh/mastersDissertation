#!/usr/bin/python
#
# This class simulates the log entries generated by Hadoop
#
# Example of Hadoop log files:
# hadoop-marcelo-tasktracker-shamrock074.log
# hadoop-marcelo-jobtracker-shamrock074.log

import os
import logging

# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4

class Logger(object):
    def __init__(self, name, filename):
        logger = logging.getLogger('log_namespace.%s' % name)
        logger.setLevel(logging.DEBUG)
        if not logger.handlers:
            handler = logging.FileHandler(filename)
            formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
            handler.setFormatter(formatter)
            handler.setLevel(logging.DEBUG)
            logger.addHandler(handler)
	self._logger = logger

    def get(self):
	return self._logger

class HadoopLogger(object):
    JOBTRACKER = "org.apache.hadoop.mapred.JobTracker"
    TASKTRACKER = "org.apache.hadoop.mapred.TaskTracker"
    JOBINPROGRESS = "org.apache.hadoop.mapred.JobInProgress"
    CLIENTTRACE = "org.apache.hadoop.mapred.TaskTracker.clienttrace"
    
    def __init__(self, hostname, username, module):
        name = "hadoop-" + username + "-" + module + "-" + hostname
        filename = os.path.join("./hadoop/logs/", '%s.log' % name)
        self._logger = Logger(name, filename).get()

    def startup(self, host):
	message = "STARTUP_MSG:   host = %s/%s" % (host, host)
	self._logger.info(message)

    def task_start(self, taskID):
        # "2013-08-20 21:19:07,892 INFO org.apache.hadoop.mapred.JobTracker: Removing task 'attempt_201308202026_0002_m_000005_0'"
        message = self.TASKTRACKER + ": JVM with ID: jvm_000000000000_0000_x_-0000000000 given task: " + taskID;
        self._logger.info(message)

    def task_finish(self, taskID):
        # "2013-08-20 21:18:04,804 INFO org.apache.hadoop.mapred.TaskTracker: Task attempt_201308202026_0001_m_000004_0 is done.";
        message = self.TASKTRACKER + ": Task " + taskID + " is done."
        self._logger.info(message)

    def shuffle_intent(self, taskID, partitions):
        # "2013-08-20 21:18:31,737 INFO org.apache.hadoop.mapred.TaskTracker: SHUFFLE_INTENT, Task: attempt_201308202026_0002_m_000003_0, partitions: [102324, 2223424, 3232323, 423323]"
        message = self.TASKTRACKER + ": SHUFFLE_INTENT, Task: " + taskID + ", partitions: ["
        for p in partitions:
            message += "%d, " % p
        message = message[:-2]
        message += "]"
        self._logger.info(message)

    def shuffle_finish(self, srcIP, srcPort, dstIP, dstPort, size, mapper, duration, reducer):
        # "2013-08-20 21:18:31,737 INFO org.apache.hadoop.mapred.TaskTracker.clienttrace: src: 172.27.102.31:50060, dest: 172.27.102.34:43726, bytes: 201113591, op: MAPRED_SHUFFLE, cliID: attempt_201308202026_0002_m_000003_0, duration: 3873490658, reducer: 1";
        message = self.CLIENTTRACE + ": src: " + srcIP + ":" + str(srcPort) + ", dest: " + dstIP + ":" + str(dstPort) + ", bytes: " + str(size)+ ", op: MAPRED_SHUFFLE, cliID: " + mapper + ", duration: " + str(duration) + ", reducer: " + str(int(reducer.split("_")[4]))
        self._logger.info(message)
    
    def job_start(self, job_id, num_maps, num_reduces):
        # "2013-08-20 21:17:43,540 INFO org.apache.hadoop.mapred.JobInProgress: job_201308202026_0001: nMaps=4 nReduces=0 max=-1"
        message = self.JOBINPROGRESS + ": " + job_id + ": nMaps=" + str(num_maps) + " nReduces=" + str(num_reduces) + " max=-1"
        self._logger.info(message)

    def job_finish(self, job_id):
        # "2013-08-20 21:18:04,906 INFO org.apache.hadoop.mapred.JobInProgress: Job job_201308202026_0001 has completed successfully.";
        message = self.JOBINPROGRESS + ": Job " + job_id + " has completed successfully."
        self._logger.info(message)

class SimulatorLogger(object):
    def __init__(self, hostname):
        name = "simulator-" + hostname
        filename = os.path.join("./hadoop/logs/", '%s.log' % name)
        self._logger = Logger(name, filename).get()

    def get(self):
	return self._logger
